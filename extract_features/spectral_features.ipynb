{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8de60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1f6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor:\n",
    "    def __init__(self, audio_path, samples_per_segment, num_mfcc_vectors_per_segment, data, \n",
    "                 label, num_segments=3, num_mfcc=40, num_mels=40, n_fft=2048, hop_length=512):\n",
    "        self.file_path = audio_path\n",
    "        self.signal, self.sr = librosa.load(audio_path)\n",
    "        self.samples_per_segment = samples_per_segment\n",
    "        self.num_mfcc_vectors_per_segment = num_mfcc_vectors_per_segment\n",
    "        self.num_segments = num_segments\n",
    "        self.num_mfcc = num_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.num_mels = num_mels\n",
    "        self.hop_length = hop_length\n",
    "        self.label = label\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d3cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCExtractor(AudioFeatureExtractor):\n",
    "    def extract(self):\n",
    "        for segment in range(self.num_segments):        \n",
    "            start = self.samples_per_segment * segment\n",
    "            finish = start + self.samples_per_segment\n",
    "            \n",
    "            mfcc = librosa.feature.mfcc(y=self.signal[start:finish], sr=self.sr, n_mfcc=self.num_mfcc, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "            mfcc = mfcc.T\n",
    "            \n",
    "            if len(mfcc) == self.num_mfcc_vectors_per_segment:\n",
    "                self.data[\"mfcc\"].append(mfcc.tolist())\n",
    "                self.data[\"labels\"].append(self.label-1)\n",
    "                print(\"{}, segment:{}\".format(self.file_path, segment+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed2b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpectrogramExtractor(AudioFeatureExtractor):\n",
    "    def extract(self):\n",
    "        for segment in range(self.num_segments):\n",
    "            start = self.samples_per_segment * segment\n",
    "            finish = start + self.samples_per_segment\n",
    "\n",
    "            mel_spec = librosa.feature.melspectrogram(y=self.signal[start:finish], sr=self.sr, n_fft=self.n_fft, n_mels=self.num_mels, hop_length=self.hop_length)\n",
    "            log_mel_spec = librosa.power_to_db(mel_spec).T\n",
    "\n",
    "            if len(log_mel_spec) == self.num_mfcc_vectors_per_segment:\n",
    "                self.data['mels'].append(log_mel_spec.tolist())\n",
    "                self.data[\"labels\"].append(self.label-1)\n",
    "                print(\"{}, segment:{}\".format(self.file_path, segment+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b294e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFTExtractor(AudioFeatureExtractor):\n",
    "    def extract(self):\n",
    "        for segment in range(self.num_segments):\n",
    "            start = self.samples_per_segment * segment\n",
    "            finish = start + self.samples_per_segment\n",
    "            \n",
    "            chroma_stft = librosa.feature.chroma_stft(y=self.signal[start:finish], sr=self.sr, n_chroma=self.num_mels, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "            log_chroma_stft = librosa.power_to_db(chroma_stft).T\n",
    "            \n",
    "            if len(log_chroma_stft) == self.num_mfcc_vectors_per_segment:\n",
    "                self.data[\"chroma_stft\"].append(log_chroma_stft.tolist())\n",
    "                self.data[\"labels\"].append(self.label-1)\n",
    "                print(\"{}, segment:{}\".format(self.file_path, segment+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e789be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQTExtractor(AudioFeatureExtractor):\n",
    "    def extract(self):\n",
    "        for segment in range(self.num_segments):\n",
    "            start = self.samples_per_segment * segment\n",
    "            finish = start + self.samples_per_segment\n",
    "            try:\n",
    "                chroma_cqt = librosa.feature.chroma_cqt(y=self.signal[start:finish], sr=self.sr, n_chroma=self.num_mels, bins_per_octave=80)\n",
    "                log_chroma_cqt = librosa.power_to_db(chroma_cqt).T\n",
    "\n",
    "                if len(log_chroma_cqt) == self.num_mfcc_vectors_per_segment:\n",
    "                    self.data[\"chroma_cqt\"].append(log_chroma_cqt.tolist())\n",
    "                    self.data[\"labels\"].append(self.label-1)\n",
    "                    print(\"{}, segment:{}\".format(self.file_path, segment+1))\n",
    "            except: \n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3a484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_extractor(feature:str):\n",
    "    if feature == \"mfcc\":\n",
    "        return MFCCExtractor\n",
    "    if feature == \"mels\":\n",
    "        return MelSpectrogramExtractor\n",
    "    if feature == \"chroma_cqt\":\n",
    "        return CQTExtractor\n",
    "    if feature == \"chroma_stft\":\n",
    "        return STFTExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c41d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc(dataset_path, feature, num_segments=3, hop_length=512):\n",
    "    \n",
    "    data = {\n",
    "              'emotion':[], \n",
    "               feature:[], \n",
    "              'labels':[] \n",
    "           }\n",
    "    \n",
    "    samples_per_segment = int(samples_per_signal / num_segments)\n",
    "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "\n",
    "    print(f'Extracting {feature}', end='')\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        \n",
    "        if dirpath is not dataset_path:\n",
    "\n",
    "            emotion = dirpath.split(\"\\\\\")[-1]\n",
    "            data[\"emotion\"].append(emotion)\n",
    "            print(\"\\nProcessing: {}\".format(emotion))\n",
    "\n",
    "            for file in filenames:\n",
    "                file_path = os.path.join(dirpath, file)\n",
    "                extractor = create_feature_extractor(feature)(file_path, samples_per_segment, num_mfcc_vectors_per_segment, data, label=i)\n",
    "                extractor.extract()\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = # pathr to audio data -> r\"D:\\SER-RO-MAHA\\Sample Speech Data\"\n",
    "sample_rate = 22050\n",
    "duration = 3 #in seconds\n",
    "samples_per_signal = sample_rate * duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "387b9af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting chroma_cqt\n",
      "Processing: Anger\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Anger\\03-01-05-01-01-01-01.wav, segment:1\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Anger\\JE_a15.wav, segment:1\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Anger\\JE_a15.wav, segment:2\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Anger\\JE_a15.wav, segment:3\n",
      "\n",
      "Processing: Disgust\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Disgust\\03-01-07-01-01-01-01.wav, segment:1\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Disgust\\03-01-07-01-01-01-01.wav, segment:2\n",
      "\n",
      "Processing: Fear\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Fear\\03-01-06-01-01-01-01.wav, segment:1\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Fear\\03-01-06-01-01-01-01.wav, segment:2\n",
      "\n",
      "Processing: Happy\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Happy\\03-01-03-01-01-01-01.wav, segment:1\n",
      "\n",
      "Processing: Neutral\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Neutral\\03-01-01-01-01-01-01.wav, segment:1\n",
      "\n",
      "Processing: Sad\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Sad\\03-01-04-01-01-01-01.wav, segment:1\n",
      "\n",
      "Processing: Surprise\n",
      "D:\\SER-RO-MAHA\\Sample Speech Data\\Surprise\\03-01-08-01-01-01-01.wav, segment:1\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mfcc_data = save_mfcc(dataset_path, feature='mfcc')\n",
    "json_path = 'mfcc_data.json'\n",
    "\n",
    "with open(json_path, 'w') as fp:\n",
    "    json.dump(mfcc_data, fp, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
