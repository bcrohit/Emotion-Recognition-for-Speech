{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "KjrNZv8bjsHA",
   "metadata": {
    "id": "KjrNZv8bjsHA"
   },
   "source": [
    "# **Speech Emotion Recogntion**\n",
    "\n",
    "Training phase.\n",
    "Out of all the clasification algorithms trained, *'MLP Classifier'* (which was not expected) turned out to be the better performing algorith with an accuracy around *84%* when scaled and transformed using *'Robust Scaler'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C10B_9uljbVm",
   "metadata": {
    "id": "C10B_9uljbVm"
   },
   "source": [
    "### **Importing Neccessary Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7cc27",
   "metadata": {
    "id": "41e7cc27"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer, QuantileTransformer, StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OlEk7-H5jKHh",
   "metadata": {
    "id": "OlEk7-H5jKHh"
   },
   "source": [
    "## **All Other Stuff**\n",
    "\n",
    "Preparing data and declaring functions so that it makes things easier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QNUFTV_6JV2Q",
   "metadata": {
    "id": "QNUFTV_6JV2Q"
   },
   "source": [
    "### **Save the Model and Hyperparameters**\n",
    "\n",
    "If the model is a neural network we have to save it in '*hdf5*' format, hence the below function takes a optional argument '*NN*' which denotes 'Neural Networks'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kADtwmsDYHBJ",
   "metadata": {
    "id": "kADtwmsDYHBJ"
   },
   "outputs": [],
   "source": [
    "def save_model(model, model_path, hyperparameters=None, param_path=None, NN=False):\n",
    "    if NN:\n",
    "        model.save(model_path)\n",
    "        return\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(hyperparameters, param_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zYpcPGlk_BQi",
   "metadata": {
    "id": "zYpcPGlk_BQi"
   },
   "source": [
    "### **Load The Data**\n",
    "\n",
    "The features extracted were stores in '*json*' format. This below function loads feature vectors with the shape (rows, n_mels, columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hk4EfwBHfLi0",
   "metadata": {
    "id": "hk4EfwBHfLi0"
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, feature):\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    X = np.array(data[feature])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    print(\"Data succesfully loaded!\")\n",
    "\n",
    "    return  X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3HIYM7BsfLgm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3HIYM7BsfLgm",
    "outputId": "380678fe-9398-4c4e-e514-2e4a76468514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data succesfully loaded!\n",
      "Data succesfully loaded!\n",
      "Data succesfully loaded!\n",
      "Data succesfully loaded!\n",
      "Data succesfully loaded!\n",
      "Data succesfully loaded!\n"
     ]
    }
   ],
   "source": [
    "xMels_rs, yMels_rs = load_data(r\"/content/drive/MyDrive/Data/mel_spec_data_40.json\", 'mel_spec')\n",
    "xMfcc_rs, yMfcc_rs = load_data(\"/content/drive/MyDrive/Data/mfcc_data_40.json\", 'mfcc')\n",
    "xMels_tess, yMels_tess = load_data(\"/content/drive/MyDrive/Data/tess_mel_spec_40.json\", 'mel_spec')\n",
    "xMfcc_tess, yMfcc_tess = load_data(\"/content/drive/MyDrive/Data/tess_mfcc_40.json\", 'mfcc')\n",
    "xStft, yStft = load_data(\"/content/drive/MyDrive/Data/Tess_chroma_stft_40.json\", 'chroma_stft')\n",
    "xCqt, yCqt = load_data('/content/drive/MyDrive/Data/Tess_chroma_cqt_40.json', 'chroma_cqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iWvNXCmh_IDo",
   "metadata": {
    "id": "iWvNXCmh_IDo"
   },
   "source": [
    "### **Transform the Data**\n",
    "\n",
    "MFCC and MEl Spectrogram features of RAVDESS and SAVEE audio files were extracted with '*n_mels=26*' whereas all of TESS audio files features were extracted with '*n_mels=40*'. Hence, there will be a mismatch of shape which would generate a error. The below function would fill the missing columns with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dMRYNpXpfLeG",
   "metadata": {
    "id": "dMRYNpXpfLeG"
   },
   "outputs": [],
   "source": [
    "def transform_data(data, no_of_columns=18):\n",
    "    empty_rows = np.empty((no_of_columns, data.shape[2]))\n",
    "    empty_rows[:] = 0\n",
    "    x = []\n",
    "    for i in range(len(data)):\n",
    "    arr = np.concatenate((data[i], empty_rows), axis=0)\n",
    "    x.append(arr)\n",
    "    return np.array(x)\n",
    "\n",
    "xMels_rs = transform_data(xMels_rs)\n",
    "xMfcc_rs = transform_data(xMfcc_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aX5G9K2MG5_",
   "metadata": {
    "id": "5aX5G9K2MG5_"
   },
   "source": [
    "While working with the MLP Classifier in thr 'sklearn' module and other traditional ML models, the input features shold be a 2D array of vectors. This below functions convert the given 3D array to 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2N_rIFyrfLXI",
   "metadata": {
    "id": "2N_rIFyrfLXI"
   },
   "outputs": [],
   "source": [
    "def convert_to_2d(X, Y):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(X)):\n",
    "        x.append(X[i].flatten())\n",
    "        # x.append(X[i].flatten())\n",
    "        \n",
    "        y.append(Y[i])\n",
    "        # y.append(Y[i])\n",
    "        \n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "xMels_rs_2d, yMels_rs_2d = convert_to_2d(xMels_rs, yMels_rs)\n",
    "xMfcc_rs_2d, yMfcc_rs_2d = convert_to_2d(xMfcc_rs, yMfcc_rs)\n",
    "xMels_tess_2d, yMels_tess_2d = convert_to_2d(xMels_tess, yMels_tess)\n",
    "xMfcc_tess_2d, yMfcc_tess_2d = convert_to_2d(xMfcc_tess, yMfcc_tess)\n",
    "xStft_2d, yStft_2d = convert_to_2d(xStft, yStft)\n",
    "xCqt_2d, yCqt_2d = convert_to_2d(xCqt, yCqt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlTRiuhS-sFC",
   "metadata": {
    "id": "zlTRiuhS-sFC"
   },
   "source": [
    "### **Scaling and Transforming Data**\n",
    "\n",
    "This below function scales the given 2D array with the given transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZPTBWem_NI6S",
   "metadata": {
    "id": "ZPTBWem_NI6S"
   },
   "outputs": [],
   "source": [
    "def scale_2d_array(array, transformer):\n",
    "    scaler = transformer\n",
    "    scaled_input = scaler.fit_transform(array)\n",
    "    return scaled_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AjccMOjBGxvp",
   "metadata": {
    "id": "AjccMOjBGxvp"
   },
   "source": [
    "This below function scales the given 2D array with the given transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hf0jrfPFGrzo",
   "metadata": {
    "id": "Hf0jrfPFGrzo"
   },
   "outputs": [],
   "source": [
    "def scale_3d_data(X, transformer):\n",
    "    scaler = transformer\n",
    "    scaled_arr = scaler.fit_transform(X.reshape(-1, X.shape[-1]))\n",
    "    scaled_arr = scaled_arr.reshape(*X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zxwxs6Ea-meX",
   "metadata": {
    "id": "Zxwxs6Ea-meX"
   },
   "source": [
    "### **Preparing the 2D Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b45e3",
   "metadata": {
    "id": "c92b45e3"
   },
   "outputs": [],
   "source": [
    "def prepare_2d_dataset(X, y, transformer):\n",
    "    scaled_input = scale_2d_array(X, transformer)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(scaled_input, y, random_state=43, test_size=0.2)\n",
    "    return xTrain, xTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wdYXtIM-WHQE",
   "metadata": {
    "id": "wdYXtIM-WHQE"
   },
   "source": [
    "### **Making Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RDO4QQQXUCLR",
   "metadata": {
    "id": "RDO4QQQXUCLR"
   },
   "outputs": [],
   "source": [
    "def predict_on_ml_models(model, X, y):\n",
    "    predictions = model.predict(X)\n",
    "    print(f'Accuracy: {accuracy_score(y, predictions)}')\n",
    "    confusion_matrix(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V1WRkzraCACt",
   "metadata": {
    "id": "V1WRkzraCACt"
   },
   "outputs": [],
   "source": [
    "def predict_on_nn_models(model, X, y):\n",
    "    # add a dimension to input data for sample - model.predict() expects a 4d array in this case\n",
    "    X = X[np.newaxis, ...] # array shape (1, 130, 13, 1)\n",
    "\n",
    "    # perform prediction\n",
    "    prediction = model.predict(X)\n",
    "\n",
    "    # get index with max value\n",
    "    predicted_index = np.argmax(prediction, axis=1)\n",
    "\n",
    "    print(\"Target: {}, Predicted label: {}\".format(y, predicted_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ARd9q09rWXKk",
   "metadata": {
    "id": "ARd9q09rWXKk"
   },
   "source": [
    "## **Training Traditional ML Models**\n",
    "\n",
    "Trained on Random Forest Classifier and MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dRTdXFbN_4Jb",
   "metadata": {
    "id": "dRTdXFbN_4Jb"
   },
   "outputs": [],
   "source": [
    "# Creating Inputs and Output\n",
    "\n",
    "input_features = np.concatenate((xMels_rs_2d, xMfcc_rs_2d, xMels_tess_2d, xMfcc_tess_2d, xStft_2d, xCqt_2d))\n",
    "target = np.concatenate((yMels_rs_2d, yMfcc_rs_2d, yMels_tess_2d, yMfcc_tess_2d, yStft_2d, yCqt_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ym4Ld7m5-VnA",
   "metadata": {
    "id": "ym4Ld7m5-VnA"
   },
   "source": [
    "### **MLP Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ce76b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "603ce76b",
    "outputId": "e690e618-eab6-4620-c741-c6da5c1460db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48014440433212996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_model = MLPClassifier(alpha=0.01, batch_size=32, \n",
    "                      epsilon=1e-08, hidden_layer_sizes=(300,), \n",
    "                      solver='adam', activation='relu',\n",
    "                      verbose=True, warm_start=True,\n",
    "                      early_stopping=True,\n",
    "                      learning_rate='adaptive', max_iter=30)\n",
    "\n",
    "# train the model\n",
    "xTrain, xTest, yTrain, yTest = prepare_2d_dataset(input_features, target, RobustScaler())\n",
    "mlp_model.fit(xTrain, yTrain)\n",
    "\n",
    "# save the model\n",
    "model_path = '/content/drive/MyDrive/Models/mlp_model.pkl'\n",
    "param_path = '/content/drive/MyDrive/Models/mlp_params.pkl'\n",
    "save_model(mlp_model, model_path, mlp_model.get_params(), param_path)\n",
    "\n",
    "# make predictions\n",
    "predict_on_ml_models(mlp_model, xTest, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z301O3Pn-Iri",
   "metadata": {
    "id": "Z301O3Pn-Iri"
   },
   "source": [
    "### **Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764e017",
   "metadata": {
    "id": "4764e017"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=250, max_depth=20, min_samples_split=200, \n",
    "                                 min_samples_leaf=100, class_weight='balanced', random_state=2)\n",
    "\n",
    "\n",
    "# train the model\n",
    "xTrain, xTest, yTrain, yTest = prepare_2d_dataset(input_features, target, PowerTransformer())\n",
    "rnd_clf.fit(xTrain, yTrain)\n",
    "\n",
    "# save the model\n",
    "model_path = '/content/drive/MyDrive/Models/rfc_model_pow.joblib'\n",
    "param_path = '/content/drive/MyDrive/Models/rfc_param_pow.pkl'\n",
    "save_model(rnd_clf, model_path, hyperparameters=rnd_clf.get_params(), param_path=param_path, NN=False)\n",
    "\n",
    "# make predictions\n",
    "predict_on_ml_models(rnd_clf, xTest, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sdN6ArLVhPRK",
   "metadata": {
    "id": "sdN6ArLVhPRK"
   },
   "source": [
    "## **Training ANN Models**\n",
    "\n",
    "> I have used CNN and LSTM models with three hidden layers. To reduce over fitting l2 regularization is done.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902IIfE2h6Gk",
   "metadata": {
    "id": "902IIfE2h6Gk"
   },
   "source": [
    "### **Computing Class Weights**\n",
    "\n",
    "Since some of the neural networks does not implicitly calculate the weights of the class we need to calculate class weigths and pass it as a hyperparameter explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27LmV-UYh5cY",
   "metadata": {
    "id": "27LmV-UYh5cY"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(target), y=target)\n",
    "class_weights = {0: class_weights[0], 1: class_weights[1], 2: class_weights[2], 3: class_weights[3], \n",
    "                 4: class_weights[4], 5: class_weights[5], 6: class_weights[6]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u6nvNtx0HYF8",
   "metadata": {
    "id": "u6nvNtx0HYF8"
   },
   "source": [
    "### **Prepare 3D Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kJXyIRAUFiCr",
   "metadata": {
    "id": "kJXyIRAUFiCr"
   },
   "outputs": [],
   "source": [
    "def prepare_ann_datasets(test_size, validation_size, transformer, lstm=False):\n",
    "\n",
    "    features = scale_3d_data(input_features, transformer)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_size)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "\n",
    "    if lstm:\n",
    "      return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0TO9Y7ueTf7",
   "metadata": {
    "id": "f0TO9Y7ueTf7"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, axis = plt.subplots(2)\n",
    "\n",
    "    axis[0].plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "    axis[0].plot(history.history[\"val_accuracy\"], label='Validation Accuracy')\n",
    "    axis[0].legend()\n",
    "\n",
    "    axis[1].plot(history.history[\"loss\"], label=\"Train Error\")\n",
    "    axis[1].plot(history.history[\"val_loss\"], label=\"Test Error\")\n",
    "    axis[1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8yVAE36vIJOw",
   "metadata": {
    "id": "8yVAE36vIJOw"
   },
   "source": [
    "### **Create Inputs and Targets**\n",
    "\n",
    "All the features extracted individually must be concatenated so as to pass it the model being trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VuWEyuI0IC5a",
   "metadata": {
    "id": "VuWEyuI0IC5a"
   },
   "outputs": [],
   "source": [
    "input_features = np.concatenate((xMels_rs, xMfcc_rs, xMels_tess, xMfcc_tess, xStft, xCqt))\n",
    "target = np.concatenate((yMels_rs, yMfcc_rs, yMels_tess, yMfcc_tess, yStft, yCqt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7TAO7FNnCfeR",
   "metadata": {
    "id": "7TAO7FNnCfeR"
   },
   "source": [
    "### **LSTM Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wT6lvMfTCdTM",
   "metadata": {
    "id": "wT6lvMfTCdTM"
   },
   "outputs": [],
   "source": [
    "def construct_lstm_model():\n",
    "    # create network\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2]) # 130, 13\n",
    "\n",
    "    # build network topology\n",
    "    lstm_model = keras.Sequential()\n",
    "\n",
    "    # 2 LSTM layers\n",
    "    lstm_model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True))\n",
    "    lstm_model.add(keras.layers.LSTM(64))\n",
    "\n",
    "    # dense layer\n",
    "    lstm_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    lstm_model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "    # output layer\n",
    "    lstm_model.add(keras.layers.Dense(7, activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    lstm_model.compile(optimizer=optimiser,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    lstm_model.summary()\n",
    "\n",
    "    # train model\n",
    "    history = lstm_model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30, class_weight=class_weights)\n",
    "    return lstm_model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bIC8piuECm1j",
   "metadata": {
    "id": "bIC8piuECm1j"
   },
   "source": [
    "### **CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rtg9FyTvChfr",
   "metadata": {
    "id": "Rtg9FyTvChfr"
   },
   "outputs": [],
   "source": [
    "def construct_cnn_model():\n",
    "    # create network\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "\n",
    "    # build network topology\n",
    "    cnn_model = keras.Sequential()\n",
    "\n",
    "    # 1st conv layer\n",
    "    cnn_model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    cnn_model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "    cnn_model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # 2nd conv layer\n",
    "    cnn_model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    cnn_model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "    cnn_model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # 3rd conv layer\n",
    "    cnn_model.add(keras.layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "    cnn_model.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
    "    cnn_model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # flatten output and feed it into dense layer\n",
    "    cnn_model.add(keras.layers.Flatten())\n",
    "    cnn_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    cnn_model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "    # output layer\n",
    "    cnn_model.add(keras.layers.Dense(7, activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    cnn_model.compile(optimizer=optimiser,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    cnn_model.summary()\n",
    "\n",
    "    # train model\n",
    "    history = cnn_model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30, class_weight=class_weights)\n",
    "    return cnn_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YOggNrDhGYUz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOggNrDhGYUz",
    "outputId": "64852b5a-9078-4b27-e645-60065a961658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 44, 64)            26880     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64,519\n",
      "Trainable params: 64,519\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "538/538 [==============================] - 40s 67ms/step - loss: 1.7991 - accuracy: 0.2793 - val_loss: 1.5811 - val_accuracy: 0.4086\n",
      "Epoch 2/2\n",
      "538/538 [==============================] - 31s 58ms/step - loss: 1.4393 - accuracy: 0.4402 - val_loss: 1.2603 - val_accuracy: 0.5361\n"
     ]
    }
   ],
   "source": [
    "# get train, validation, test splits for lstm models\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_ann_datasets(0.25, 0.2, transformer=RobustScaler(), lstm=True)\n",
    "lstm_model, lstm_history = construct_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ujHZJ8WucwH2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujHZJ8WucwH2",
    "outputId": "a7e4a498-463a-489d-d3d7-f0c501250e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 42, 38, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 21, 19, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 21, 19, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 19, 17, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 10, 9, 32)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 10, 9, 32)        128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 9, 8, 32)          4128      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 5, 4, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 5, 4, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 640)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                41024     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,559\n",
      "Trainable params: 55,367\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "538/538 [==============================] - 45s 82ms/step - loss: 1.8720 - accuracy: 0.2709 - val_loss: 1.5205 - val_accuracy: 0.4627\n",
      "Epoch 2/2\n",
      "538/538 [==============================] - 42s 79ms/step - loss: 1.4774 - accuracy: 0.4322 - val_loss: 1.2853 - val_accuracy: 0.5496\n"
     ]
    }
   ],
   "source": [
    "# get train, validation, test splits for cnn models\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_ann_datasets(0.25, 0.2, transformer=RobustScaler())\n",
    "cnn_model, cnn_history = construct_cnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v7ZpBs2am_LS",
   "metadata": {
    "id": "v7ZpBs2am_LS"
   },
   "source": [
    "### **Model Evaluation and Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LfdaQ3tjDEXR",
   "metadata": {
    "id": "LfdaQ3tjDEXR"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, history):\n",
    "    # plot accuracy/error for training and validation\n",
    "    plot_history(history)\n",
    "\n",
    "    # evaluate model on test set\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7H2AvFEwm22B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7H2AvFEwm22B",
    "outputId": "e1cb360e-7dce-4530-df6f-8ab44de032e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Target: 6, Predicted label: [3]\n"
     ]
    }
   ],
   "source": [
    "evaluate(cnn_model, cnn_history)\n",
    "X_to_predict = X_test[1]\n",
    "y_to_predict = y_test[1]\n",
    "\n",
    "# predict sample\n",
    "predict_on_nn_models(cnn_model, X_to_predict, y_to_predict)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
